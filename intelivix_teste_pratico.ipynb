{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 1\n",
    "\n",
    "Carregamento da base de dados no MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from urllib import request\n",
    "\n",
    "# Função responsável por carregar o dataset\n",
    "def load_dataset(dataset):\n",
    "    datasets = ['dataset-0', 'dataset-1', 'dataset-2',\n",
    "                'dataset-3', 'dataset-4', 'dataset-5']\n",
    "    \n",
    "    if dataset not in datasets:\n",
    "        raise Exception(\"Dataset não encontrado.\")\n",
    "    \n",
    "    try:\n",
    "        with open('./datasets/'+ dataset +'.json', 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    except IOError:\n",
    "        if not os.path.exists('./datasets'):\n",
    "            os.mkdir('datasets')\n",
    "        \n",
    "        print(\"Baixando o %s...\" % dataset)\n",
    "        \n",
    "        request.urlretrieve('https://s3.amazonaws.com/intelivix-datasets/testes_praticos/'+ dataset +'.json',\n",
    "                            './datasets/'+ dataset +'.json')\n",
    "        \n",
    "        return load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('localhost', 27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.intelivix_teste_pratico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('dataset-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f9f4eba1308>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.processos.insert_many(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 2\n",
    "\n",
    "Respostas das consultas no MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantidade total de processos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.processos.count_documents(filter={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantidade total de andamentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215633"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(processo['andamentos']) for processo in db.processos.find()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantidade de processos por estado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RJ: 186\n",
      "AL: 181\n",
      "AP: 182\n",
      "MG: 193\n",
      "SE: 169\n",
      "AM: 168\n",
      "RS: 162\n",
      "BA: 202\n",
      "PR: 169\n",
      "SC: 175\n",
      "ES: 207\n",
      "PI: 183\n",
      "RO: 192\n",
      "MA: 203\n",
      "SP: 189\n",
      "CE: 186\n",
      "PA: 178\n",
      "RN: 198\n",
      "PE: 188\n",
      "AC: 172\n",
      "PB: 208\n",
      "DF: 196\n",
      "RR: 187\n",
      "TO: 194\n",
      "MS: 166\n",
      "MT: 182\n",
      "GO: 184\n"
     ]
    }
   ],
   "source": [
    "qntd_processos_estados = db.processos.aggregate([{'$group': {'_id': '$estado', 'qntd': {'$sum': 1}}}])\n",
    "\n",
    "for estado in qntd_processos_estados:\n",
    "    print(\"%s: %s\" % (estado['_id'], estado['qntd']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantidade de juízes cujo nome começa com 'S':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "db.processos.count_documents(filter={'juiz': {'$regex': re.compile(r'^S')}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantidade de ocorrências de cada etiqueta (da mais popular para a menos popular):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White: 45238\n",
      "Yellow: 45094\n",
      "Orange: 45083\n",
      "Blue: 45079\n",
      "Pink: 45069\n",
      "Cyan: 45048\n",
      "Red: 44981\n",
      "Brown: 44978\n",
      "Green: 44967\n",
      "Purple: 44897\n",
      "Magenta: 44891\n",
      "Beige: 44860\n",
      "Black: 44752\n"
     ]
    }
   ],
   "source": [
    "qntd_etiquetas = {}\n",
    "\n",
    "for processo in db.processos.find():\n",
    "    for andamento in processo['andamentos']:\n",
    "        for etiqueta in set(andamento['etiquetas']):\n",
    "            if etiqueta in qntd_etiquetas.keys():\n",
    "                qntd_etiquetas[etiqueta] += 1\n",
    "                \n",
    "            else:\n",
    "                qntd_etiquetas[etiqueta] = 1\n",
    "\n",
    "qntd_etiquetas = sorted(qntd_etiquetas.items(), key=lambda etiqueta: etiqueta[1], reverse=True)\n",
    "\n",
    "for etiqueta, qntd in qntd_etiquetas:\n",
    "    print(\"%s: %s\" % (etiqueta, qntd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuração da conexão do SQLAlchemy com o Postgres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "engine = create_engine('postgres://postgres:postgres@localhost:5432/intelivix_teste_pratico')\n",
    "\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação dos modelos Processo e Andamento com o SQLAlchemy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship\n",
    "from sqlalchemy import Column, Integer, String, DateTime, Sequence, ForeignKey\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Processo(Base):\n",
    "    __tablename__ = 'processos'\n",
    "    \n",
    "    id = Column(String, primary_key=True)\n",
    "    npu = Column(String, nullable=True)\n",
    "    estado = Column(String, nullable=True)\n",
    "    spider = Column(String, nullable=True)\n",
    "    juiz = Column(String, nullable=True)\n",
    "    data_distribuicao = Column(DateTime, nullable=True)\n",
    "    data_captura = Column(DateTime, nullable=True)\n",
    "    \n",
    "    def __repr__(self):        \n",
    "        return \"<Processo(id='%s', npu='%s', estado='%s', spider='%s', juiz='%s', \" \\\n",
    "               \"data_distribuicao='%s', data_captura='%s')>\" % (\n",
    "                self.id, self.npu, self.estado, self.spider, self.juiz,\n",
    "                self.data_distribuicao.isoformat(), self.data_captura.isoformat())\n",
    "\n",
    "\n",
    "class Andamento(Base):\n",
    "    __tablename__ = 'andamentos'\n",
    "    \n",
    "    id = Column(Integer, Sequence('andamento_id_seq'), primary_key=True)\n",
    "    processo_id = Column(String, ForeignKey(Processo.id))\n",
    "    texto = Column(String, nullable=True)\n",
    "    data = Column(DateTime, nullable=True)\n",
    "    etiquetas = Column(String, nullable=True)\n",
    "    processo = relationship(Processo)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<Andamento(id='%s', processo_id='%s', texto='%s', \" \\\n",
    "               \"data='%s', etiquetas='%s')>\" % (\n",
    "                self.id, self.processo_id, self.texto, self.data.isoformat(),\n",
    "                self.etiquetas)\n",
    "\n",
    "\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers para a inserção dos processos e andamentos no banco de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Função responsável por converter uma string em DateTime\n",
    "def to_datetime(datetime_str):\n",
    "    return datetime.strptime(datetime_str, '%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# Função reponsável por transformar uma lista de etiquetas em uma string com as etiquetas separadas por vírgulas\n",
    "def format_etiquetas(etiquetas):\n",
    "    return ','.join(etiquetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inserção dos processos e andamentos no Postgres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for processo in db.processos.find():\n",
    "    session.add(\n",
    "        Processo(\n",
    "            id=processo['id'],\n",
    "            npu=processo['npu'],\n",
    "            estado=processo['estado'],\n",
    "            spider=processo['spider'],\n",
    "            juiz=processo['juiz'],\n",
    "            data_distribuicao=to_datetime(processo['data_distribuicao']),\n",
    "            data_captura=to_datetime(processo['data_captura'])\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    session.commit()\n",
    "    \n",
    "    for andamento in processo['andamentos']:\n",
    "        session.add(\n",
    "            Andamento(\n",
    "                processo_id=processo['id'],\n",
    "                texto=andamento['texto'],\n",
    "                data=to_datetime(andamento['data']),\n",
    "                etiquetas=format_etiquetas(andamento['etiquetas'])\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers para as transformações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função reponsável pela contagem dos nomes dos juízes\n",
    "def count_names(full_name):\n",
    "    name_splited = full_name.split(' ')\n",
    "    \n",
    "    return len(name_splited)\n",
    "\n",
    "# Função responsável por retornar apenas o primeiro e o último nome do juiz\n",
    "def first_and_last_name(full_name):\n",
    "    name_splited = full_name.split(' ')\n",
    "    \n",
    "    return \"%s %s\" % (name_splited[0], name_splited[-1])\n",
    "\n",
    "# Função responsável por verificar se um determinado NPU é inválido\n",
    "def is_invalid_npu(npu):\n",
    "    ano = int(npu.split('.')[1])\n",
    "    \n",
    "    if ano < 1980 or ano > 2018:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Função responsável por tornar um NPU válido\n",
    "def transform_npu(npu):\n",
    "    npu_splited = npu.split('.')\n",
    "    npu_head = npu_splited[0]\n",
    "    npu_tail = '.'.join(npu_splited[2:])\n",
    "    \n",
    "    return \"%s.2000.%s\" % (npu_head, npu_tail)\n",
    "\n",
    "# Função responsável por contar quantas palavras começam com um determinado caractere\n",
    "def count_words_with_ch(text, ch, case_sensitive=False):\n",
    "    if case_sensitive:\n",
    "        return sum(word.startswith(ch) for word in text.split())\n",
    "    \n",
    "    return sum(word.startswith(ch.lower()) for word in text.lower().split())\n",
    "\n",
    "# Função responsável por remover as palavras que começam com um determinado caractere\n",
    "def remove_words_with_ch(text, ch, case_sensitive=False):\n",
    "    if case_sensitive:\n",
    "        return ' '.join(word for word in text.split() if not word.startswith(ch))\n",
    "    \n",
    "    return ' '.join(word for word in text.lower().split() if not word.startswith(ch.lower()))\n",
    "\n",
    "# Função responsável por verificar se uma determinada palavra está contida em um texto\n",
    "def word_in_text(word, text, case_sensitive=False):\n",
    "    if case_sensitive:\n",
    "        return word in text\n",
    "    \n",
    "    return word.lower() in text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alteração no nome dos juízes para deixar apenas o primeiro e último nome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for processo in session.query(Processo).all():\n",
    "    if count_names(processo.juiz) > 2:\n",
    "        processo.juiz = first_and_last_name(processo.juiz) \n",
    "\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remoção dos andamentos cuja data é anterior a data de distribuição:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.query(Andamento).filter(Andamento.processo_id == Processo.id,\n",
    "                                Andamento.data < Processo.data_distribuicao\n",
    "                               ).delete(synchronize_session=False)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifição dos npus que não possuem um ano entre 1980 e 2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for processo in session.query(Processo).all():\n",
    "    if is_invalid_npu(processo.npu):\n",
    "        processo.npu = transform_npu(processo.npu)\n",
    "\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remoção das palavras que começam com a letra 'r' dos textos dos andamentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for andamento in session.query(Andamento).all():\n",
    "    if count_words_with_ch(andamento.texto, 'r') > 0:\n",
    "        andamento.texto = remove_words_with_ch(andamento.texto, 'r')\n",
    "\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação da coluna 'qntd_andamentos' na tabela 'processos' e 'cinema_no_texto' na tabela 'andamentos' e inserção dos seus respectivos valores:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Migração feita pelo Alembic para criar a coluna 'qntd_andamentos' na tabela 'processos' e 'cinema_no_texto' na tabela 'andamentos':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Running upgrade  -> 391fcb150ba2, add qntd_andamentos\n",
      "INFO  [alembic.runtime.migration] Running upgrade 391fcb150ba2 -> 75633146d7e9, add cinema_no_texto\n"
     ]
    }
   ],
   "source": [
    "!alembic upgrade head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atualização dos modelos após a migração feita pelo Alembic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Boolean\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Processo(Base):\n",
    "    __tablename__ = 'processos'\n",
    "    \n",
    "    id = Column(String, primary_key=True)\n",
    "    npu = Column(String, nullable=True)\n",
    "    estado = Column(String, nullable=True)\n",
    "    spider = Column(String, nullable=True)\n",
    "    juiz = Column(String, nullable=True)\n",
    "    data_distribuicao = Column(DateTime, nullable=True)\n",
    "    data_captura = Column(DateTime, nullable=True)\n",
    "    qntd_andamentos = Column(Integer, nullable=True)\n",
    "    \n",
    "    def __repr__(self):        \n",
    "        return \"<Processo(id='%s', npu='%s', estado='%s', spider='%s', juiz='%s', \" \\\n",
    "               \"data_distribuicao='%s', data_captura='%s', qntd_andamentos='%s')>\" % (\n",
    "                self.id, self.npu, self.estado, self.spider, self.juiz,\n",
    "                self.data_distribuicao.isoformat(), self.data_captura.isoformat(), self.qntd_andamentos)\n",
    "\n",
    "\n",
    "class Andamento(Base):\n",
    "    __tablename__ = 'andamentos'\n",
    "    \n",
    "    id = Column(Integer, Sequence('andamento_id_seq'), primary_key=True)\n",
    "    processo_id = Column(String, ForeignKey(Processo.id))\n",
    "    texto = Column(String, nullable=True)\n",
    "    data = Column(DateTime, nullable=True)\n",
    "    etiquetas = Column(String, nullable=True)\n",
    "    cinema_no_texto = Column(Boolean, nullable=True)\n",
    "    processo = relationship(Processo)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<Andamento(id='%s', processo_id='%s', texto='%s', data='%s', \" \\\n",
    "               \"etiquetas='%s', cinema_no_texto='%s')>\" % (\n",
    "                self.id, self.processo_id, self.texto, self.data.isoformat(),\n",
    "                self.etiquetas, self.cinema_no_texto)\n",
    "\n",
    "\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserção da quantidade de andamentos de cada processo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for processo in session.query(Processo).all():\n",
    "    processo.qntd_andamentos = session.query(Andamento).filter(Andamento.processo_id == processo.id).count()\n",
    "\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificação dos andamentos cujo texto contém a palavra 'cinema' para a inserção no banco de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for andamento in session.query(Andamento).all():\n",
    "    if word_in_text('cinema', andamento.texto):\n",
    "        andamento.cinema_no_texto = True\n",
    "    \n",
    "    else:\n",
    "        andamento.cinema_no_texto = False\n",
    "\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respostas das consultas pós-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantidade total de processos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.query(Processo).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantidade total de andamentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107054"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.query(Andamento).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processo que possui a maior quantidade de andamentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Processo(id='fb15c3f4-2dcd-d021-0acc-ef13576f45ee', npu='5985515-82.2000.5.57.9825', estado='GO', spider='esaj-ce', juiz='Randee David', data_distribuicao='1993-07-09T19:32:45', data_captura='2017-05-13T12:30:36', qntd_andamentos='80')>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import desc\n",
    "\n",
    "session.query(Processo).order_by(desc(Processo.qntd_andamentos)).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Andamentos cujos textos possuem as maiores quantidades de caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1º lugar:\n",
      "\n",
      "<Andamento(id='126288', processo_id='b57aff1c-8bcb-01e2-3be7-07416f2f0ae0', texto='comparison comparison capability mercury persons drew thai advertising bond mission basics self suck presidential christian engagement correction powerful symbol singer credit those purple videos pixels seasonal pop expense stars misc icons prove important canvas mobility student alerts fee seminar expected office tape stock caused could combination gnu conference box manager more buildings unfortunately python issues attempts inc yesterday dynamic france prevent caused pda boat library processed optional cottage plug integrity trip keyword blvd sale projects happy antonio falls follows identity giving houston misc pipe investment history ward protein classification freeware contributions semester mercedes vacuum material charlotte indoor victoria cf bondage draw defined educational ericsson motorcycle special indianapolis onto trials functionality africa manchester cb distribution globe america moments certainly attend vista objects life progressive marks impact ended partnership kill military included in notebooks might bags bikini blonde daniel freedom integration southwest hp popular tutorials winners disorder couples dark mice pin describe spyware theater good centres architecture easy airlines proposal starring numerous commons shooting tim firms attached little outcome popularity examined channel information parker msn impact intelligence phentermine tools printable united sm arabia arrow finding specialist bristol iso beds showtimes warner instant dancing online navigate accepted asset easy latex hearing currently literature appointment entered economy class current top accurate pack error conferences sierra christian checkout approaches advertisement specifications extensive sensitive positions walter threshold islamic sublime november payments weak designers sum physicians anniversary sight developed analysis aug dance weak anne air getting grounds teach situations ps high elected through shoulder preferences answer converted declared wedding confidence principle alphabetical samuel ultra column magazine lamps fitness increase debian tiger whatever browsing puts tiger myself downtown selecting never hold boards smith montana zoom founded sydney accident export next newspaper graphics texts ds avoid forgotten higher by congress agricultural korean personals sunday direction bank lamp alerts eventually liable subscribers breaking okay virgin hunt measures matters canadian tony boston expressed telecom evans bt fame crowd large literature shaved obligation beef tight intervention webcam natural prior demonstrate qualifications dc google implement committees las', data='2015-10-27T01:31:40', etiquetas='Red,Purple,Black,Magenta', cinema_no_texto='False')>\n",
      "\n",
      "<Processo(id='b57aff1c-8bcb-01e2-3be7-07416f2f0ae0', npu='1763740-55.2000.1.74.5380', estado='RJ', spider='projudi-ms', juiz='Kathaleen Cohen', data_distribuicao='2006-07-05T08:48:15', data_captura='2016-05-04T22:58:51', qntd_andamentos='33')>\n",
      "\n",
      "2º lugar:\n",
      "\n",
      "<Andamento(id='187345', processo_id='f7998b9c-1456-b87d-b7a6-2ea076797483', texto='million tutorials mention someone emissions display approximately crazy flight dick copy musical cvs easily scholarship outcome sciences title host forecast interaction gallery charges columns simpson title sight american du alaska hunt might miller wheel vacuum ai battery cartridges giant tons climate fish household afraid stored cited powder applied dust visible nikon experimental ideas surveys parameter guy typically highest governments heads eng increases polyphonic changed torture cartoon making computers training awesome supreme philip taught chance enlarge national quick note twelve beast mathematics mind verify pt mine aid designated female sport speaking ability application echo ben idaho owned biography sure neighborhood identify dj managed consult juan january current minimum argentina scientific tiffany legs careers thursday him penalty adoption left only syntax annotation ages feedback butt turned bargain hop feed trying season jackson excellence pierre collectables dark weddings blade und logical jefferson attorneys division pda cells computing deaths sustainable primarily associate quite candidate orlando treat apartments perfectly medication timing guidelines moderator built bandwidth cream coastal killing supplier experienced garage pocket os james growing code jay essay wrestling distribution lived coast open toll maryland spaces goes compression california existence interactive age actual directions standard enjoy field ceiling bb payment loop wheel personnel announced limitations brief afraid dating platinum electronics efforts named xml winners featured excellent tickets contractors diesel premier tiger simulation february checks suggests preview light town issues oklahoma band establish wallpapers sell engine bargain forum platform permanent stupid peripherals edwards change bb despite swiss garden welfare circumstances accessibility oriented standing odds problems databases funny window expertise sigma circumstances nude tion periods sense publication singapore jennifer managers singles newest creative warren leading compression climate hits transactions interaction gr excess mark outdoors master messages variation susan manual ir bath effectiveness issued cleaner craft shown classifieds combat allowing others charles clean specific updated greg suit email economic broadcast laser affect qty daily analyst muscle slots fax sterling win spiritual cheats efforts upskirt consultants wed thursday sleeping settlement toronto mostly dave lawyer family pleased cycling debt itself delaware experiences scientists fear churches metro friday bad ethernet', data='2003-10-15T20:42:46', etiquetas='Brown,White,Pink', cinema_no_texto='False')>\n",
      "\n",
      "<Processo(id='f7998b9c-1456-b87d-b7a6-2ea076797483', npu='6094006-74.2000.5.46.3151', estado='RS', spider='esaj-pa', juiz='Charles Francis', data_distribuicao='1993-05-06T08:44:56', data_captura='2016-03-21T18:24:01', qntd_andamentos='69')>\n",
      "\n",
      "3º lugar:\n",
      "\n",
      "<Andamento(id='152961', processo_id='68795a85-f6af-8ad6-441c-093a003a1311', texto='dildos instant basics created telecom mile whole hospitality vacation display forms tropical evolution thru called gulf simple people normally gnu maybe camcorder admin nine hp sale themes drawing utah basically compatible albums boy and commission ave listed classes slightly correctly experimental suitable transportation overall net valley utc eat toshiba commerce second favor kenya accuracy hook addition grid linking wildlife correction determination described increasing server saudi provided vertical house considering permission instance classics energy collections lower excerpt suitable cal authentication preferences entitled temple brown tired victim positions unlike gary ourselves fetish card lowest israel suite projects string bell participation nba von administration difficult stay teenage personnel quotes heading users packs investment simple throat often lines cottage property addressed team port programmes wallpaper wing appointed newly clay suggested yourself every certain nebraska land platform vancouver generated meters perspective worst first growing episode parks given hist hate vintage gratis spears pension kernel sport narrow nhl taste dos individuals nipples fourth personalized yield intro tripadvisor for output gordon given word hollywood attributes tennessee sounds churches sexy fantasy bibliography starts identify certification manufacturers giving el war although happening scripts governor weak producing chosen mysql subject nw bachelor harris textbooks checkout concern muscle nose wright pill annotation national bishop nights mask overnight atlanta builder told tape conventional ordinary hs direction globe forward campus ben exceed traffic licensed messages fair presents switches doors housewares pearl google molecular sole greatest cuba vbulletin likely focus losses crime diary developed incest kernel tournament diving commands cp countries madison tv wholesale colorado owner islamic fair marriott circumstances fully physician literacy boxes charger variable nfl centre merchandise together chicago advantage sounds arrangements write effectiveness midnight dose internal corn taking feeling speed denied districts pet pen advertising ps indiana suddenly diego assessment bulletin specified bears embedded accommodations antiques albany guinea tel volunteer provision xp getting judges stable dolls nor flight tramadol implementation skip knife expertise spending alcohol topic contracts hawaii bed material vision facility light organisations johnson ent provision preferences taken undergraduate sons mechanism networking born', data='2011-03-28T10:15:10', etiquetas='White,Orange,White,Pink,Purple', cinema_no_texto='False')>\n",
      "\n",
      "<Processo(id='68795a85-f6af-8ad6-441c-093a003a1311', npu='1041999-64.2000.6.17.0294', estado='RN', spider='esaj-rr', juiz='Wilford Parker', data_distribuicao='2000-08-05T15:08:05', data_captura='2016-01-23T16:26:14', qntd_andamentos='44')>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import func\n",
    "\n",
    "andamentos = session.query(Andamento).order_by(desc(func.char_length(Andamento.texto))).all()[:3]\n",
    "\n",
    "for n, andamento in enumerate(andamentos):\n",
    "    print(\"%sº lugar:\\n\" % (n + 1))\n",
    "    print(andamento)\n",
    "    print()\n",
    "    print(andamento.processo)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Andamento mais antigo com o termo 'cinema':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Andamento(id='42201', processo_id='dcc3d0eb-7a96-eab0-af82-27e70c5c8436', texto='compliance inclusive venture southwest activity modem specialty litigation plant pizza notice euro wi contributions mitchell mp holes jewellery ir grey carl highlight poor trademark departments feed seem verify gen messaging archive cancer bath arrangement css lawn battery organizational arms camcorders kingdom tramadol generally ja contemporary basic mg bestiality flexibility benefits von story commerce filing alert furthermore km sony cinema practice checking bar generic beta contest que moment dr ordered trunk counts vista taxes watching fund wing over schedule newsletter lease stats northeast growth spread balance lebanon serves temple martin chemicals liability amazon begin pulled generic uk somewhere stones informational spa cool limits so dimension aluminum schedules preliminary stream adopt later jackson votes fail north disabilities nh merchant ng cloud solutions only san loss edition usa era occasion communities heads administrators sure continuing catalogue party php jay obviously instructor philips biological begin prev networks title que housewares johnny hiking promoting throat waters cellular guest vision glory appearance em cleaning own eggs fashion gr cl potentially part private citysearch whether enabled west commonwealth abroad setting ii soft inc solution deadline carolina browser covering secrets domestic origin se affiliate pre sport url via ha contemporary earth michael charged operate puerto jim continuing capabilities faq indiana passing outline programmes manner deliver loves sexo airline identify polls singer approval built portable sensitive wrestling illegal balance cooper ea container cloud is moral dragon census capability served keeping simpson permits theoretical notebooks cable like offline flying park states certainly archived given services party billy three ice parties hb shaved suites flags phil wear passion affect tutorial given increasing city excess determine academic university gcc fed win victoria huge labels don hundreds species diseases bandwidth headquarters tough corner trace poland dropped jesus shots mice involving given loved powers shots filters billy baseball toshiba central blade persons si company cumshot vital hours targeted employer nj secure bargains pet toyota slowly michelle bank consult isp structure signs complaint premier provided hp advantage environment', data='1993-05-07T09:07:19', etiquetas='Beige,Orange,Cyan,Magenta,Pink', cinema_no_texto='True')>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import asc\n",
    "\n",
    "session.query(Andamento).filter(Andamento.cinema_no_texto == True).order_by(asc(Andamento.data)).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processo que possui o maior número formado pelos 6 primeiros números do seu npu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Processo(id='5adf3f05-ba82-d011-dcfd-014193e9b559', npu='9996389-06.2000.2.16.2519', estado='MA', spider='projudi-rj', juiz='Ayanna Aguilar', data_distribuicao='2011-07-20T09:53:28', data_captura='2017-07-12T23:47:39', qntd_andamentos='15')>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_number_found = 0\n",
    "\n",
    "for processo in session.query(Processo):\n",
    "    first_6_numbers = int(processo.npu[:6])\n",
    "    \n",
    "    if first_6_numbers > highest_number_found:\n",
    "        highest_number_found = first_6_numbers\n",
    "        processo_id = processo.id\n",
    "\n",
    "session.query(Processo).filter(Processo.id == processo_id).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mês/ano que foram capturados mais processos para cada \"spider\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contagem da quantidade de processos capturados em cada mês/ano para cada \"spider\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiders = {}\n",
    "\n",
    "for processo in session.query(Processo).all():\n",
    "    if processo.spider not in spiders:\n",
    "        spiders[processo.spider] = {}\n",
    "        \n",
    "    if processo.data_captura.strftime('%m/%Y') not in spiders[processo.spider]:\n",
    "        spiders[processo.spider][processo.data_captura.strftime('%m/%Y')] = 1\n",
    "    \n",
    "    else:\n",
    "        spiders[processo.spider][processo.data_captura.strftime('%m/%Y')] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções que filtram, ordenam e, por fim, retornam a data com o maior número de processos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função responsável por retornar o total de anos de uma data no formato mês/ano\n",
    "def date_to_years(date):\n",
    "    date_splited = date.split('/')\n",
    "    month = int(date_splited[0])\n",
    "    year = int(date_splited[1])\n",
    "    \n",
    "    return year + month / 12\n",
    "\n",
    "# Função responsável por retornar a data com o maior número de processos\n",
    "def most_popular_date(qntd_por_data):\n",
    "    # Retira todos os meses/anos cuja quantidade de processos é diferente da quantidade máxima\n",
    "    qntd_por_data = [(data, qntd_por_data[data]) for data in qntd_por_data\n",
    "                     if qntd_por_data[data] == max(qntd_por_data.values())]\n",
    "\n",
    "    # Ordena da data mais recente para a menos recente    \n",
    "    qntd_por_data = sorted(qntd_por_data, key=lambda x: date_to_years(x[0]), reverse=True)\n",
    "\n",
    "    return qntd_por_data[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mês/ano com o maior número de processos capturados para cada \"spider\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pje-pi: 10/2018\n",
      "projudi-rr: 05/2018\n",
      "esaj-pe: 07/2018\n",
      "pje-pr: 10/2018\n",
      "projudi-pr: 08/2018\n",
      "esaj-df: 11/2018\n",
      "esaj-es: 04/2016\n",
      "projudi-pa: 06/2018\n",
      "esaj-mt: 07/2018\n",
      "pje-mt: 02/2018\n",
      "projudi-rj: 12/2017\n",
      "projudi-ac: 04/2018\n",
      "esaj-am: 12/2017\n",
      "esaj-ma: 04/2018\n",
      "esaj-to: 12/2018\n",
      "pje-se: 03/2018\n",
      "projudi-al: 06/2017\n",
      "esaj-pr: 02/2018\n",
      "projudi-ms: 05/2018\n",
      "projudi-mg: 11/2018\n",
      "esaj-se: 03/2018\n",
      "projudi-rn: 02/2018\n",
      "projudi-sp: 02/2016\n",
      "pje-sp: 12/2018\n",
      "esaj-ce: 01/2016\n",
      "projudi-ba: 03/2018\n",
      "pje-pb: 09/2018\n",
      "esaj-sc: 10/2018\n",
      "esaj-sp: 08/2018\n",
      "pje-rr: 03/2018\n",
      "pje-to: 01/2016\n",
      "pje-sc: 12/2016\n",
      "projudi-am: 01/2016\n",
      "projudi-se: 08/2016\n",
      "pje-ac: 07/2018\n",
      "projudi-go: 12/2018\n",
      "esaj-ms: 07/2016\n",
      "esaj-rs: 10/2016\n",
      "esaj-go: 06/2018\n",
      "pje-ma: 11/2016\n",
      "projudi-ro: 04/2017\n",
      "esaj-rn: 03/2018\n",
      "esaj-pi: 04/2018\n",
      "projudi-to: 10/2018\n",
      "pje-es: 12/2016\n",
      "pje-ba: 09/2018\n",
      "projudi-sc: 11/2017\n",
      "esaj-pa: 08/2018\n",
      "projudi-ce: 12/2018\n",
      "esaj-pb: 03/2018\n",
      "pje-mg: 06/2018\n",
      "pje-rs: 08/2018\n",
      "projudi-rs: 08/2016\n",
      "projudi-df: 06/2017\n",
      "pje-df: 01/2018\n",
      "projudi-mt: 03/2016\n",
      "esaj-ba: 04/2016\n",
      "pje-al: 09/2017\n",
      "esaj-mg: 05/2018\n",
      "pje-rn: 05/2016\n",
      "pje-ce: 08/2016\n",
      "pje-ms: 04/2018\n",
      "pje-am: 08/2016\n",
      "projudi-ma: 09/2017\n",
      "esaj-rr: 03/2017\n",
      "pje-pe: 03/2018\n",
      "projudi-pe: 10/2017\n",
      "projudi-pb: 01/2018\n",
      "projudi-ap: 02/2016\n",
      "pje-ro: 10/2016\n",
      "esaj-rj: 05/2018\n",
      "esaj-al: 12/2017\n",
      "esaj-ro: 05/2018\n",
      "projudi-es: 08/2018\n",
      "esaj-ac: 05/2018\n",
      "pje-rj: 02/2017\n",
      "esaj-ap: 03/2018\n",
      "projudi-pi: 12/2018\n",
      "pje-pa: 04/2018\n",
      "pje-go: 03/2018\n",
      "pje-ap: 11/2017\n"
     ]
    }
   ],
   "source": [
    "for spider, qntd_por_data in spiders.items():\n",
    "    print(\"%s: %s\" % (spider, most_popular_date(qntd_por_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 4\n",
    "\n",
    "Exportação das tabelas do Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Função responsável por criar o arquivo csv\n",
    "def create_csv(filename, header, content):\n",
    "    with open(filename, 'w') as f:\n",
    "        csvfile = csv.writer(f, delimiter='|', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "        \n",
    "        csvfile.writerow(header)\n",
    "        csvfile.writerows(content)\n",
    "        \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportação da tabela 'processos' para o arquivo processos.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "processos_header = ['id', 'npu', 'estado', 'spider', 'juiz', 'data_distribuicao',\n",
    "                    'data_captura', 'qntd_andamentos']\n",
    "\n",
    "processos_content = [[processo.id, processo.npu, processo.estado, processo.spider, processo.juiz,\n",
    "                      processo.data_distribuicao.isoformat(), processo.data_captura.isoformat(),\n",
    "                      processo.qntd_andamentos] for processo in session.query(Processo).all()]\n",
    "\n",
    "create_csv('processos.csv', processos_header, processos_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportação da tabela 'andamentos' para o arquivo andamentos.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "andamentos_header = ['id', 'processo_id', 'texto', 'data', 'etiquetas', 'cinema_no_texto']\n",
    "\n",
    "andamentos_content = [[andamento.id, andamento.processo_id, andamento.texto,\n",
    "                       andamento.data.isoformat(), andamento.etiquetas,\n",
    "                       andamento.cinema_no_texto] for andamento in session.query(Andamento).all()]\n",
    "\n",
    "create_csv('andamentos.csv', andamentos_header, andamentos_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o Github não permite arquivos maiores que 100MB, coloquei o arquivo processos.csv e o arquivo andamentos.csv no Google Drive.\n",
    "\n",
    "* [processos.csv](https://drive.google.com/open?id=1YDq3GjKD3zqOtVxBj0T9IQvlAG_yACL5)\n",
    "* [andamentos.csv](https://drive.google.com/open?id=1TFQUep7sqX8yBlNTkqSC09QEA85O44R0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
