{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelivix Teste Prático\n",
    "\n",
    "Este notebook contem as etapas realizadas para o Teste Prático de Engenharia, que é destinado aos candidatos a desenvolvedor no time de \"Engenharia de Dados\" da Intelivix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 1\n",
    "\n",
    "Carregamento da base de dados no MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from urllib import request\n",
    "\n",
    "# Função responsável por carregar o dataset\n",
    "def load_dataset(dataset):\n",
    "    datasets = ['dataset-0', 'dataset-1', 'dataset-2',\n",
    "                'dataset-3', 'dataset-4', 'dataset-5']\n",
    "    \n",
    "    if dataset not in datasets:\n",
    "        raise Exception(\"Dataset não encontrado.\")\n",
    "    \n",
    "    try:\n",
    "        with open('./datasets/'+ dataset +'.json', 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    except IOError:\n",
    "        if not os.path.exists('./datasets'):\n",
    "            os.mkdir('datasets')\n",
    "        \n",
    "        print(\"Baixando o %s...\" % dataset)\n",
    "        \n",
    "        request.urlretrieve('https://s3.amazonaws.com/intelivix-datasets/testes_praticos/'+ dataset +'.json',\n",
    "                            './datasets/'+ dataset +'.json')\n",
    "        \n",
    "        return load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('localhost', 27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.intelivix_teste_pratico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('dataset-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f49595af108>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.processos.insert_many(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 2\n",
    "\n",
    "Respostas das consultas no MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantidade total de processos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.processos.count_documents(filter={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantidade total de andamentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215633"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(processo['andamentos']) for processo in db.processos.find()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantidade de processos por estado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RJ: 186\n",
      "AL: 181\n",
      "AP: 182\n",
      "MG: 193\n",
      "SE: 169\n",
      "AM: 168\n",
      "RS: 162\n",
      "BA: 202\n",
      "PR: 169\n",
      "SC: 175\n",
      "ES: 207\n",
      "PI: 183\n",
      "RO: 192\n",
      "MA: 203\n",
      "SP: 189\n",
      "CE: 186\n",
      "PA: 178\n",
      "RN: 198\n",
      "PE: 188\n",
      "AC: 172\n",
      "PB: 208\n",
      "DF: 196\n",
      "RR: 187\n",
      "TO: 194\n",
      "MS: 166\n",
      "MT: 182\n",
      "GO: 184\n"
     ]
    }
   ],
   "source": [
    "qntd_processos_estados = db.processos.aggregate([{'$group': {'_id': '$estado', 'qntd': {'$sum': 1}}}])\n",
    "\n",
    "for estado in qntd_processos_estados:\n",
    "    print(\"%s: %s\" % (estado['_id'], estado['qntd']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantidade de juízes cujo nome começa com 'S':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "db.processos.count_documents(filter={'juiz': {'$regex': re.compile(r'^S')}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantidade de ocorrências de cada etiqueta (da mais popular para a menos popular):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White: 45238\n",
      "Yellow: 45094\n",
      "Orange: 45083\n",
      "Blue: 45079\n",
      "Pink: 45069\n",
      "Cyan: 45048\n",
      "Red: 44981\n",
      "Brown: 44978\n",
      "Green: 44967\n",
      "Purple: 44897\n",
      "Magenta: 44891\n",
      "Beige: 44860\n",
      "Black: 44752\n"
     ]
    }
   ],
   "source": [
    "qntd_etiquetas = {}\n",
    "\n",
    "for processo in db.processos.find():\n",
    "    for andamento in processo['andamentos']:\n",
    "        for etiqueta in set(andamento['etiquetas']):\n",
    "            if etiqueta in qntd_etiquetas.keys():\n",
    "                qntd_etiquetas[etiqueta] += 1\n",
    "                \n",
    "            else:\n",
    "                qntd_etiquetas[etiqueta] = 1\n",
    "\n",
    "qntd_etiquetas = sorted(qntd_etiquetas.items(), key=lambda etiqueta: etiqueta[1], reverse=True)\n",
    "\n",
    "for etiqueta, qntd in qntd_etiquetas:\n",
    "    print(\"%s: %s\" % (etiqueta, qntd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuração da conexão do SQLAlchemy com o Postgres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "engine = create_engine('postgres://postgres:postgres@localhost:5432/intelivix_teste_pratico')\n",
    "\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação dos modelos Processo e Andamento com o SQLAlchemy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship\n",
    "from sqlalchemy import Column, Integer, String, DateTime, Sequence, ForeignKey\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Processo(Base):\n",
    "    __tablename__ = 'processos'\n",
    "    \n",
    "    id = Column(String, primary_key=True)\n",
    "    npu = Column(String, nullable=True)\n",
    "    estado = Column(String, nullable=True)\n",
    "    spider = Column(String, nullable=True)\n",
    "    juiz = Column(String, nullable=True)\n",
    "    data_distribuicao = Column(DateTime, nullable=True)\n",
    "    data_captura = Column(DateTime, nullable=True)\n",
    "    \n",
    "    def __repr__(self):        \n",
    "        return \"<Processo(id='%s', npu='%s', estado='%s', spider='%s', juiz='%s', \" \\\n",
    "               \"data_distribuicao='%s', data_captura='%s')>\" % (\n",
    "                self.id, self.npu, self.estado, self.spider, self.juiz,\n",
    "                self.data_distribuicao.isoformat(), self.data_captura.isoformat())\n",
    "\n",
    "\n",
    "class Andamento(Base):\n",
    "    __tablename__ = 'andamentos'\n",
    "    \n",
    "    id = Column(Integer, Sequence('andamento_id_seq'), primary_key=True)\n",
    "    processo_id = Column(String, ForeignKey(Processo.id))\n",
    "    texto = Column(String, nullable=True)\n",
    "    data = Column(DateTime, nullable=True)\n",
    "    etiquetas = Column(String, nullable=True)\n",
    "    processo = relationship(Processo)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<Andamento(id='%s', processo_id='%s', texto='%s', data='%s', etiquetas='%s')>\" % (\n",
    "                self.id, self.processo_id, self.texto, self.data.isoformat(), self.etiquetas)\n",
    "\n",
    "\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers para a inserção dos processos e andamentos no banco de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Função responsável por converter uma string em DateTime\n",
    "def to_datetime(datetime_str):\n",
    "    return datetime.strptime(datetime_str, '%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# Função reponsável por transformar uma lista de etiquetas em uma string com as etiquetas separadas por vírgulas\n",
    "def format_etiquetas(etiquetas):\n",
    "    return ','.join(etiquetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inserção dos processos e andamentos no Postgres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for processo in db.processos.find():\n",
    "    session.add(\n",
    "        Processo(\n",
    "            id=processo['id'],\n",
    "            npu=processo['npu'],\n",
    "            estado=processo['estado'],\n",
    "            spider=processo['spider'],\n",
    "            juiz=processo['juiz'],\n",
    "            data_distribuicao=to_datetime(processo['data_distribuicao']),\n",
    "            data_captura=to_datetime(processo['data_captura'])\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    session.commit()\n",
    "    \n",
    "    for andamento in processo['andamentos']:\n",
    "        session.add(\n",
    "            Andamento(\n",
    "                processo_id=processo['id'],\n",
    "                texto=andamento['texto'],\n",
    "                data=to_datetime(andamento['data']),\n",
    "                etiquetas=format_etiquetas(andamento['etiquetas'])\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers para as transformações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função reponsável pela contagem dos nomes dos juízes\n",
    "def count_names(full_name):\n",
    "    name_splited = full_name.split(' ')\n",
    "    \n",
    "    return len(name_splited)\n",
    "\n",
    "# Função responsável por retornar apenas o primeiro e o último nome do juiz\n",
    "def first_and_last_name(full_name):\n",
    "    name_splited = full_name.split(' ')\n",
    "    \n",
    "    return \"%s %s\" % (name_splited[0], name_splited[-1])\n",
    "\n",
    "# Função responsável por verificar se um determinado NPU é inválido\n",
    "def is_invalid_npu(npu):\n",
    "    ano = int(npu.split('.')[1])\n",
    "    \n",
    "    if ano < 1980 or ano > 2018:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Função responsável por tornar um NPU válido\n",
    "def transform_npu(npu):\n",
    "    npu_splited = npu.split('.')\n",
    "    npu_head = npu_splited[0]\n",
    "    npu_tail = '.'.join(npu_splited[2:])\n",
    "    \n",
    "    return \"%s.2000.%s\" % (npu_head, npu_tail)\n",
    "\n",
    "# Função responsável por contar quantas palavras começam com um determinado caractere\n",
    "def count_words_with_ch(text, ch, case_sensitive=False):\n",
    "    if case_sensitive:\n",
    "        return sum(word.startswith(ch) for word in text.split())\n",
    "    \n",
    "    return sum(word.startswith(ch.lower()) for word in text.lower().split())\n",
    "\n",
    "# Função responsável por remover as palavras que começam com um determinado caractere\n",
    "def remove_words_with_ch(text, ch, case_sensitive=False):\n",
    "    if case_sensitive:\n",
    "        return ' '.join(word for word in text.split() if not word.startswith(ch))\n",
    "    \n",
    "    return ' '.join(word for word in text.lower().split() if not word.startswith(ch.lower()))\n",
    "\n",
    "# Função responsável por verificar se uma determinada palavra está contida em um texto\n",
    "def word_in_text(word, text, case_sensitive=False):\n",
    "    if case_sensitive:\n",
    "        return word in text\n",
    "    \n",
    "    return word.lower() in text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alteração no nome dos juízes para deixar apenas o primeiro e último nome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for processo in session.query(Processo).all():\n",
    "    if count_names(processo.juiz) > 2:\n",
    "        processo.juiz = first_and_last_name(processo.juiz) \n",
    "\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remoção dos andamentos cuja data é anterior a data de distribuição:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.query(Andamento).filter(Andamento.processo_id == Processo.id,\n",
    "                                Andamento.data < Processo.data_distribuicao\n",
    "                               ).delete(synchronize_session=False)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifição dos npus que não possuem um ano entre 1980 e 2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for processo in session.query(Processo).all():\n",
    "    if is_invalid_npu(processo.npu):\n",
    "        processo.npu = transform_npu(processo.npu)\n",
    "\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remoção das palavras que começam com a letra 'r' nos textos dos andamentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for andamento in session.query(Andamento).all():\n",
    "    if count_words_with_ch(andamento.texto, 's') > 0:\n",
    "        andamento.texto = remove_words_with_ch(andamento.texto, 's')\n",
    "\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação da coluna 'qntd_andamentos' na tabela 'processos' e 'cinema_no_texto' na tabela 'andamentos' e inserção dos seus respectivos valores:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Migração feita pelo Alembic para criar a coluna 'qntd_andamentos' na tabela 'processos' e 'cinema_no_texto' na tabela 'andamentos':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Running upgrade  -> 391fcb150ba2, add qntd_andamentos\n",
      "INFO  [alembic.runtime.migration] Running upgrade 391fcb150ba2 -> 75633146d7e9, add cinema_no_texto\n"
     ]
    }
   ],
   "source": [
    "!alembic upgrade head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atualização dos modelos após a migração feita pelo Alembic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Boolean\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Processo(Base):\n",
    "    __tablename__ = 'processos'\n",
    "    \n",
    "    id = Column(String, primary_key=True)\n",
    "    npu = Column(String, nullable=True)\n",
    "    estado = Column(String, nullable=True)\n",
    "    spider = Column(String, nullable=True)\n",
    "    juiz = Column(String, nullable=True)\n",
    "    data_distribuicao = Column(DateTime, nullable=True)\n",
    "    data_captura = Column(DateTime, nullable=True)\n",
    "    qntd_andamentos = Column(Integer, nullable=True)\n",
    "    \n",
    "    def __repr__(self):        \n",
    "        return \"<Processo(id='%s', npu='%s', estado='%s', spider='%s', juiz='%s', \" \\\n",
    "               \"data_distribuicao='%s', data_captura='%s', qntd_andamentos='%s')>\" % (\n",
    "                self.id, self.npu, self.estado, self.spider, self.juiz,\n",
    "                self.data_distribuicao.isoformat(), self.data_captura.isoformat(), self.qntd_andamentos)\n",
    "\n",
    "\n",
    "class Andamento(Base):\n",
    "    __tablename__ = 'andamentos'\n",
    "    \n",
    "    id = Column(Integer, Sequence('andamento_id_seq'), primary_key=True)\n",
    "    processo_id = Column(String, ForeignKey(Processo.id))\n",
    "    texto = Column(String, nullable=True)\n",
    "    data = Column(DateTime, nullable=True)\n",
    "    etiquetas = Column(String, nullable=True)\n",
    "    cinema_no_texto = Column(Boolean, nullable=True)\n",
    "    processo = relationship(Processo)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<Andamento(id='%s', processo_id='%s', texto='%s', data='%s', \" \\\n",
    "               \"etiquetas='%s', cinema_no_texto='%s')>\" % (\n",
    "                self.id, self.processo_id, self.texto, self.data.isoformat(),\n",
    "                self.etiquetas, self.cinema_no_texto)\n",
    "\n",
    "\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserção da quantidade de andamentos de cada processo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for processo in session.query(Processo).all():\n",
    "    processo.qntd_andamentos = session.query(Andamento).filter(Andamento.processo_id == processo.id).count()\n",
    "\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificação dos andamentos cujo texto contem a palavra 'cinema' para a inserção no banco de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for andamento in session.query(Andamento).all():\n",
    "    if word_in_text('cinema', andamento.texto):\n",
    "        andamento.cinema_no_texto = True\n",
    "    \n",
    "    else:\n",
    "        andamento.cinema_no_texto = False\n",
    "\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respostas das consultas pós-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantidade total de processos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.query(Processo).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantidade total de andamentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107054"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.query(Andamento).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processo que possui a maior quantidade de andamentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Processo(id='fb15c3f4-2dcd-d021-0acc-ef13576f45ee', npu='5985515-82.2000.5.57.9825', estado='GO', spider='esaj-ce', juiz='Randee David', data_distribuicao='1993-07-09T19:32:45', data_captura='2017-05-13T12:30:36', qntd_andamentos='80')>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import desc\n",
    "\n",
    "session.query(Processo).order_by(desc(Processo.qntd_andamentos)).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Andamentos cujos textos possuem as maiores quantidades de caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1º lugar:\n",
      "\n",
      "<Andamento(id='54172', processo_id='21e4a282-1541-f27f-e75c-4976b8422547', texto='encoding icon findings delivered files aside language cad back entities amber les row bodies jun noted consult german faqs eggs massage environments impossible featuring outline availability high blonde con he line require oracle receiver returns engineering then rt ones pot templates grid quality debate expense theoretical trees phones talks granted licence methodology pubmed outdoors river awarded themselves fail parents white charlotte realized cooper included apply keeps despite jesus thou jelsoft apart ireland marked editors factory waves fonts championship cutting printable pan investor prevention organic accounting combination cricket founded priorities playing limits union ethernet interesting rv wings corner baskets deadline disorders residential regulation position baseball relatively beads talking institutions palace voting reality oriented empire feelings wheels enter provide franklin traditional editorial ah las avenue tokyo tracker foreign diagnosis healing continued trailers nashville cup abroad intellectual hawaii apartment christian pursuant includes component guns contribute firewall costa hp broadway talent fell onto designed nations hosting casinos risk adverse method public galleries pressure dealing pdt aims id discipline pantyhose regards latest acrobat wire degree gas increased chairman innovation precision mail meat edition nutten golden parameter noticed level adjustment provide rt feeds translate trailer yards programme mexican however motors automated louisiana quite bulgaria agent laptops consists photos invited conditions thomas background restoration cooperation charges jennifer liberal getting ct heroes room russian loans evans wonderful exercise processors changed emotional philosophy mail breaking existing mechanism particularly range reduction locator cloud atlantic catalogue dutch minimum race notification vendors extensions total matthew basket fri excel implementing robin grown payday eric tape business interpretation estimates assume lift anything awards bio technology thumbnail purposes town okay adopt nashville computer metro qty concentration paris girl frame infant particularly certain compensation trading blues wine developments click atom associate republican body ts admission boxes falls throat goods especially desert carolina carbon informed about league pine full australia location feet apartments exchange indicates building presentation experts anywhere connector thru utah administrator francis rivers exit thermal place tried poland haven dated hardcore line milfhunter', data='2003-04-04T13:17:59', etiquetas='Magenta,Beige,Cyan', cinema_no_texto='False')>\n",
      "\n",
      "<Processo(id='21e4a282-1541-f27f-e75c-4976b8422547', npu='0226687-66.2000.1.00.4331', estado='DF', spider='pje-pi', juiz='Ollie Drake', data_distribuicao='2001-10-30T16:04:21', data_captura='2017-12-08T15:30:43', qntd_andamentos='11')>\n",
      "\n",
      "2º lugar:\n",
      "\n",
      "<Andamento(id='200134', processo_id='979fe207-f07b-3ce4-f231-bff1ad6014cc', texto='helped associate translate wide lighting gadgets agency valley metropolitan formed weblog meters repair aside usual attitude pda town cam transfers genes cartridges term authors mary relative willing games foundation gives pass theatre gave kinds gender message hong deliver websites unknown committees criticism represented foot covers win moderate thinking directed textbooks operate handheld transfers fair chapters detroit memory rt arrangements noticed worlds key engineers billing mystery expressed bytes wallpaper good icons revision kent advantages kind creative mins carry hop normally promotions made angel interest hewlett http machinery close brian constitutes moreover whatever ocean peter help carol oklahoma bears birthday indicated procedures makes version elected gen xxx differences nipples economic given gr experiences bad image arrive can relevant tomorrow typically av europe idea mod true nuclear restaurants dispute closer evidence breasts gate write league collectables addresses kg comics thousand recommendation explorer in feeling hill na alarm jordan associations understanding warehouse interactions telephone nursing buildings horror germany packs variable channel council et divorce deep neighborhood dating warning avoid problems bringing programme di trained fellow bibliography parts potter creating construction lebanon walking arthur donation us loading disney run qualified largely nov religion film warner introduce videos canon responsibilities tall ultimately canon points prohibited exact affect foster deals folk millions adams factors desktops walked counties weekend functions announce tomorrow built fantasy origin marine navigation treatment recipe mortgage cs ross bracelet consumption tournament houses episode assignment progressive attachments usage increasingly lol eliminate dsl level closed instruction men heating networks thoughts marshall programmes dvds bulk grid president miss consolidation joint receiving characteristics edition paying captain martin postage crm releases instruments manhattan eastern bedrooms boot answered drunk insight highway far arguments forward particularly column lung virginia passage hate personals australian customized antiques institution entrance haven lives filing customer match missed trail representatives familiar either ethical hills xl puts unix lane controls resort producers agricultural detection alien monday hole coast zoom exactly interpretation disorders firms painting daniel cost always horror mini dimension physical accommodation', data='2010-11-07T15:48:36', etiquetas='Yellow,Green,Beige,Brown', cinema_no_texto='False')>\n",
      "\n",
      "<Processo(id='979fe207-f07b-3ce4-f231-bff1ad6014cc', npu='0545654-60.2000.8.64.2221', estado='TO', spider='esaj-am', juiz='Golden Kline', data_distribuicao='1996-07-30T18:11:39', data_captura='2016-02-12T02:29:02', qntd_andamentos='42')>\n",
      "\n",
      "3º lugar:\n",
      "\n",
      "<Andamento(id='86954', processo_id='113f531d-51cc-3601-9d4e-1b222dd11dcb', texto='maryland accessories lodge painting engineer qualified combination volunteer principle actor francisco winds hosts pcs awards arcade linear rick resources cassette premier donate conservation russia weekly rental reported resources infrastructure beyond poor double narrow inspection median rick properly unknown commands accounting vitamin dublin finger factory exhibition tones iraqi pub dollars judicial affect programming announcements generate rd numbers offices favorite denied clearance village bookmark bestiality louis reason management trained harry updated victoria reserve chance rhode www trade was arthur wealth insert capacity users yellow consists eligibility expectations compatibility institutional liberty castle characteristics random ensure moral clothes tracking collector juvenile andy country community likely assets allen range everybody francisco juice wet register mortgage programming charger developed prevention electric relations logos cassette type broadcasting cuts east elected mixed pizza kick theater india graphics dated discussion district composition learn operate cash location menu parameters danger term heaven conversion resource relating richmond copy idea declared entity upload bought ways chips feedback athletic notify gear louisiana officer reporter prospective arrangement representatives instantly alabama prohibited fucking institution promotion multimedia february represents belt angel era identifying vinyl berkeley constitutional databases character jason companies device memories candy cruise anyway not already towards ef ottawa exp choices punk memory cultures basket observations enhance fig nintendo gen physics buyers elsewhere useful magazines establishing equally tonight jack news proposals th percentage hosted advantage managers alone resolution tracker events battery rocks expertise covered recommendations checkout talks universities dublin treatments neither gi public financing presented whose passion blade aware in logical decisions bunch trips reaction customers empty lottery mississippi needs red diversity pci felt call mens eye processors real japanese previous or pioneer division indicated himself merely exercise exercise cd beast languages noble chemistry recommendation purchased manufacturers operation atlanta pride depth jewelry parameters outsourcing hey accessory odd laptop isp platform reality alphabetical originally teens digest is managers composition casino enrollment dvds library letter conditioning clause networking restaurants ohio toy', data='2012-12-06T08:19:34', etiquetas='Pink,White,Yellow,Purple', cinema_no_texto='False')>\n",
      "\n",
      "<Processo(id='113f531d-51cc-3601-9d4e-1b222dd11dcb', npu='1228714-44.2000.1.75.8392', estado='AC', spider='pje-rn', juiz='Pinkie Carson', data_distribuicao='2007-07-14T10:18:22', data_captura='2017-11-29T13:03:21', qntd_andamentos='9')>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import func\n",
    "\n",
    "andamentos = session.query(Andamento).order_by(desc(func.char_length(Andamento.texto))).all()[:3]\n",
    "\n",
    "for n, andamento in enumerate(andamentos):\n",
    "    print(\"%sº lugar:\\n\" % (n + 1))\n",
    "    print(andamento)\n",
    "    print()\n",
    "    print(andamento.processo)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Andamento mais antigo com o termo 'cinema':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Andamento(id='42201', processo_id='dcc3d0eb-7a96-eab0-af82-27e70c5c8436', texto='compliance inclusive venture activity modem litigation plant pizza notice euro wi contributions mitchell mp holes jewellery ir grey carl highlight poor trademark departments feed verify gen messaging archive cancer bath arrangement css lawn battery organizational arms camcorders kingdom tramadol generally ja contemporary basic mg bestiality flexibility benefits von commerce filing alert furthermore km cinema practice checking bar generic beta contest que moment dr ordered robin trunk counts vista taxes watching fund wing over newsletter lease northeast growth balance lebanon temple martin chemicals liability amazon begin pulled generic uk informational cool limits dimension aluminum ruby preliminary adopt later jackson votes fail north register radar reporting disabilities nh merchant ng cloud only loss edition usa era occasion communities heads administrators continuing catalogue party php jay obviously instructor philips biological begin prev networks title que housewares johnny hiking promoting throat waters cellular guest vision glory appearance em cleaning own eggs fashion gr cl potentially part private citysearch whether enabled west commonwealth abroad ii inc deadline carolina browser covering domestic origin affiliate pre url via republican ha contemporary earth michael charged operate ross puerto registry jim continuing capabilities faq indiana romantic responsibility passing outline programmes manner deliver loves re airline identify polls approval built portable wrestling illegal balance cooper ea container cloud is moral dragon census recommend capability keeping permits theoretical notebooks cable like offline flying park certainly archived given party billy three ice parties hb rat flags phil wear passion affect tutorial given increasing city excess revolution determine academic university gcc fed win victoria huge labels don hundreds rising diseases bandwidth headquarters tough corner trace revealed poland dropped jesus mice involving given loved powers filters billy baseball toshiba central blade refine refer persons roads company cumshot results vital hours targeted recipe employer nj bargains pet toyota michelle bank consult isp complaint premier provided hp advantage environment', data='1993-05-07T09:07:19', etiquetas='Beige,Orange,Cyan,Magenta,Pink', cinema_no_texto='True')>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import asc\n",
    "\n",
    "session.query(Andamento).filter(Andamento.cinema_no_texto == True).order_by(asc(Andamento.data)).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processo que possui o maior número formado pelos 6 primeiros números do seu npu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Processo(id='5adf3f05-ba82-d011-dcfd-014193e9b559', npu='9996389-06.2000.2.16.2519', estado='MA', spider='projudi-rj', juiz='Ayanna Aguilar', data_distribuicao='2011-07-20T09:53:28', data_captura='2017-07-12T23:47:39', qntd_andamentos='15')>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_number_found = 0\n",
    "\n",
    "for processo in session.query(Processo):\n",
    "    first_6_numbers = int(processo.npu[:6])\n",
    "    \n",
    "    if first_6_numbers > highest_number_found:\n",
    "        highest_number_found = first_6_numbers\n",
    "        processo_id = processo.id\n",
    "\n",
    "session.query(Processo).filter(Processo.id == processo_id).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mês/ano que foram capturados mais processos para cada \"spider\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contagem da quantidade de processos capturados em cada mês/ano para cada \"spider\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiders = {}\n",
    "\n",
    "for processo in session.query(Processo).all():\n",
    "    if processo.spider not in spiders:\n",
    "        spiders[processo.spider] = {}\n",
    "        \n",
    "    if processo.data_captura.strftime('%m/%Y') not in spiders[processo.spider]:\n",
    "        spiders[processo.spider][processo.data_captura.strftime('%m/%Y')] = 1\n",
    "    \n",
    "    else:\n",
    "        spiders[processo.spider][processo.data_captura.strftime('%m/%Y')] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções que filtram, ordenam e, por fim, retornam a data com maior o número de processos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função responsável por retornar o total de anos de uma data no formato mês/ano\n",
    "def date_to_years(date):\n",
    "    date_splited = date.split('/')\n",
    "    month = int(date_splited[0])\n",
    "    year = int(date_splited[1])\n",
    "    \n",
    "    return year + month / 12\n",
    "\n",
    "# Função responsável por retornar a data com o maior número de processos\n",
    "def most_popular_date(qntd_por_data):\n",
    "    \n",
    "    # Retira todos os meses/anos cuja quantidade de processos é diferente da quantidade máxima    \n",
    "    qntd_por_data = [(data, qntd_por_data[data]) for data in qntd_por_data\n",
    "                     if qntd_por_data[data] == max(qntd_por_data.values())]\n",
    "\n",
    "    # Ordena da data mais recente para a menos recente    \n",
    "    qntd_por_data = sorted(qntd_por_data, key=lambda x: date_to_years(x[0]), reverse=True)\n",
    "\n",
    "    return qntd_por_data[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mês/ano com maior número de processos capturados para cada \"spider\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pje-pi: 10/2018\n",
      "projudi-rr: 05/2018\n",
      "esaj-pe: 07/2018\n",
      "pje-pr: 10/2018\n",
      "projudi-pr: 08/2018\n",
      "esaj-df: 11/2018\n",
      "esaj-es: 04/2016\n",
      "projudi-pa: 06/2018\n",
      "esaj-mt: 07/2018\n",
      "pje-mt: 02/2018\n",
      "projudi-rj: 12/2017\n",
      "projudi-ac: 04/2018\n",
      "esaj-am: 12/2017\n",
      "esaj-ma: 04/2018\n",
      "esaj-to: 12/2018\n",
      "pje-se: 03/2018\n",
      "projudi-al: 06/2017\n",
      "esaj-pr: 02/2018\n",
      "projudi-ms: 05/2018\n",
      "projudi-mg: 11/2018\n",
      "esaj-se: 03/2018\n",
      "projudi-rn: 02/2018\n",
      "projudi-sp: 02/2016\n",
      "pje-sp: 12/2018\n",
      "esaj-ce: 01/2016\n",
      "projudi-ba: 03/2018\n",
      "pje-pb: 09/2018\n",
      "esaj-sc: 10/2018\n",
      "esaj-sp: 08/2018\n",
      "pje-rr: 03/2018\n",
      "pje-to: 01/2016\n",
      "pje-sc: 12/2016\n",
      "projudi-am: 01/2016\n",
      "projudi-se: 08/2016\n",
      "pje-ac: 07/2018\n",
      "projudi-go: 12/2018\n",
      "esaj-ms: 07/2016\n",
      "esaj-rs: 10/2016\n",
      "esaj-go: 06/2018\n",
      "pje-ma: 11/2016\n",
      "projudi-ro: 04/2017\n",
      "esaj-rn: 03/2018\n",
      "esaj-pi: 04/2018\n",
      "projudi-to: 10/2018\n",
      "pje-es: 12/2016\n",
      "pje-ba: 09/2018\n",
      "projudi-sc: 11/2017\n",
      "esaj-pa: 08/2018\n",
      "projudi-ce: 12/2018\n",
      "esaj-pb: 03/2018\n",
      "pje-mg: 06/2018\n",
      "pje-rs: 08/2018\n",
      "projudi-rs: 08/2016\n",
      "projudi-df: 06/2017\n",
      "pje-df: 01/2018\n",
      "projudi-mt: 03/2016\n",
      "esaj-ba: 04/2016\n",
      "pje-al: 09/2017\n",
      "esaj-mg: 05/2018\n",
      "pje-rn: 05/2016\n",
      "pje-ce: 08/2016\n",
      "pje-ms: 04/2018\n",
      "pje-am: 08/2016\n",
      "projudi-ma: 09/2017\n",
      "esaj-rr: 03/2017\n",
      "pje-pe: 03/2018\n",
      "projudi-pe: 10/2017\n",
      "projudi-pb: 01/2018\n",
      "projudi-ap: 02/2016\n",
      "pje-ro: 10/2016\n",
      "esaj-rj: 05/2018\n",
      "esaj-al: 12/2017\n",
      "esaj-ro: 05/2018\n",
      "projudi-es: 08/2018\n",
      "esaj-ac: 05/2018\n",
      "pje-rj: 02/2017\n",
      "esaj-ap: 03/2018\n",
      "projudi-pi: 12/2018\n",
      "pje-pa: 04/2018\n",
      "pje-go: 03/2018\n",
      "pje-ap: 11/2017\n"
     ]
    }
   ],
   "source": [
    "for spider, qntd_por_data in spiders.items():\n",
    "    print(\"%s: %s\" % (spider, most_popular_date(qntd_por_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 4\n",
    "\n",
    "Exportação das tabelas do Postgres para arquivos csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Função responsável por criar criar um arquivo csv contendo os dados da tabela\n",
    "def create_csv(filename, header, content, delimiter='|'):\n",
    "    with open(filename, 'w') as f:\n",
    "        csvfile = csv.writer(f, delimiter=delimiter)\n",
    "        \n",
    "        csvfile.writerow(header)\n",
    "        csvfile.writerows(content)\n",
    "        \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportação da tabela 'processos' para o arquivo processos.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "processos_header = ['id', 'npu', 'estado', 'spider', 'juiz',\n",
    "                    'data_distribuicao', 'data_captura', 'qntd_andamentos']\n",
    "processos_content = [[processo.id, processo.npu, processo.estado, processo.spider, processo.juiz,\n",
    "                      processo.data_distribuicao.isoformat(), processo.data_captura.isoformat(),\n",
    "                      processo.qntd_andamentos] for processo in session.query(Processo).all()]\n",
    "\n",
    "create_csv('processos.csv', processos_header, processos_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportação da tabela 'andamentos' para o arquivo andamentos.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "andamentos_header = ['id', 'processo_id', 'texto', 'data', 'etiquetas', 'cinema_no_texto']\n",
    "andamentos_content = [[andamento.id, andamento.processo_id, andamento.texto,\n",
    "                       andamento.data.isoformat(), andamento.etiquetas,\n",
    "                       andamento.cinema_no_texto] for andamento in session.query(Andamento).all()]\n",
    "\n",
    "create_csv('andamentos.csv', andamentos_header, andamentos_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
